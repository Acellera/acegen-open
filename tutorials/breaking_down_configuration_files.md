# Tutorial: Breaking down configuration files

---

In this tutorial, we will explore the structure of configuration files used in ACEGEN training scripts. These configuration files are designed to be uniform, simplifying parameter management for experiments employing different modes and algorithms.

## Configuration File Structure

Below is the general structure of our configuration file:

```yaml
# Logging configuration
...

# Environment configuration
...

# Scoring function
...

# Promptsmiles configuration
...

# Model architecture
...

# Optimizer configuration
...

# Algorithm configuration
...

# Data replay configuration
...
```

We will take the example `scripts/reinvent/config_denovo.yaml`, the config file for the novo generation using the REINVENT algorithm to break down each seaction.

## Logging configuration

This section remains consistent across all scripts:


```yaml
# Logging configuration
experiment_name: acegen # Used for logging to WandB or TensorBoard
agent_name: reinvent # Used for logging to WandB or TensorBoard
log_dir: results # Path where results will be saved
logger_backend: null  # WandB, TensorBoard, or null
seed: 101 # Set the seed of each experiment. Multiple seeds can be provided as a list for sequential experiments, e.g., [101, 102, 103]
```

## Environment configuration

Consistent across all scripts, this section allows defining the number of molecules to generate in parallel in each iteration, as well as the total size of the generated library of molecules:

```yaml
# Environment configuration
num_envs: 128 # Number of SMILES to generate in parallel
total_smiles: 10_000 # Total number of SMILES to generate
```

## Scoring function

This section is also common to all scripts. ACEGEN uses [molscore](https://github.com/MorganCThomas/MolScore) by default to define scoring functions to guide the generative agents with reinforcement learning. However, `molscore` is only an optional dependency. The `custom_task` also allows to directly use a custom scoring function defined by the user. Check our [tutorial](tutorials/adding_custom_model.md) to learn how to define and use your own scoring function.

```yaml
# Scoring function
molscore: MolOpt # All available benchmarks can be found [here][https://github.com/MorganCThomas/MolScore/tree/develop/molscore/configs]
molscore_include: null # Allows to define only a subset of the benchmark scoring function (e.g. ["Albuterol_similarity"])
custom_task: null # Requires molscore to be set to null.
```

## Promptsmiles configuration

This section is different for de novo, decorative and fragment linking scripts. To learn how to configure prompsmiles properly for each of these case, please refere to out promptsmiles [tutorial](tutorials/using_promptsmiles.md).

```yaml
# Promptsmiles configuration
prompt: null  # e.g. c1ccccc
```

## Model architecture

ACEGEN provide a few model options to select from. However, we also allow users to integrate their own models into AceGen.

A detailed guide on integrating custom models can be found in this [tutorial](tutorials/adding_custom_model.md).

```yaml
# Model architecture
model: gru # gru, lstm, or gpt2
# The default prior varies for each model. Refer to the README file in the root directory for more information.
# The default vocabulary varies for each prior. Refer to the README file in the root directory for more information.
custom_model_factory: null # Path to a custom model factory (e.g. my_module.create_model)
```

## Optimizer configuration

This section is the same in all scripts. Allows to configure the learning rate and other parameter of the training optimizer.

```yaml
# Optimizer configuration
lr: 0.0001
eps: 1.0e-08
weight_decay: 0.0
```

## Algorithm configuration

This section varies for each algorithm and allows you to define its hyperparameters. Please refer to the paper for each algorithm to learn how to modify them. Links to the papers are provided in the [README.md](https://github.com/Acellera/acegen-open) file.

```yaml
# Reinvent configuration
sigma: 120
```

## Data replay configuration

Data replay is a technique often used in reinforcement learning to improve sample efficiency and stabilize training. This section allows you to configure data replay settings for your ACEGEN training scripts.

```yaml
# Data replay configuration
experience_replay: True
replay_buffer_size: 100  # Size of the replay buffer in number of molecules
replay_batch_size: 10  # Size of the batch that will be sampled in every iteration to mix with the data generated by the RL agent.
```
