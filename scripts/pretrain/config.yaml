# Logging configuration
experiment_name: acegen
agent_name: pretrain_gru
logger_backend: null # csv, wandb, tensorboard, or null
seed: 101
log_frequency: 500

# Dataset configuration
train_dataset_path: null
tokenizer: SMILESTokenizerChEMBL # SMILESTokenizerChEMBL, SMILESTokenizerEnamine, DeepSMILESTokenizer, SELFIESTokenizer, AISTokenizer, SAFETokenizer, SmiZipTokenizer 
recompute_dataset: True # if False and dataset_log_dir contains a dataset, it will be used
dataset_log_dir: /tmp/pretrain # if recomputing dataset, save it here

# Model configuration
model: gru # gru, lstm, or gpt2
custom_model_factory: null # Path to a custom model factory (e.g. my_module.create_model)
model_log_dir: /tmp/pretrain # save model here

# Training configuration
lr: 0.0001
lr_scheduler: StepLR
lr_scheduler_kwargs:
  step_size: 1
  gamma: 1.0 # no decay
epochs: 10
batch_size: 8
randomize_smiles: False
num_test_smiles: 100
