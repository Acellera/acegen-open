# Logging configuration
experiment_name: acegen
agent_name: pretrain_gru
logger_backend: null # csv, wandb, tensorboard, or null
seed: 101
recompute_dataset: True # if False and dataset_log_dir contains a dataset, it will be used
dataset_log_dir: /tmp/pretrain
model_log_dir: /tmp/pretrain
log_frequency: 500

# Dataset configuration
train_dataset_path: null
tokenizer: SMILESTokenizer # SMILESTokenizer, SMILESTokenizer2, DeepSMILESTokenizer, SELFIESTokenizer, AISTokenizer, SAFETokenizer, SmiZipTokenizer 

# Model configuration
model: gru # gru, lstm, or gpt2

# Training configuration
lr: 0.0001
lr_scheduler: StepLR
lr_scheduler_kwargs:
  step_size: 1
  gamma: 1.0 # no decay
epochs: 10
batch_size: 8
randomize_smiles: False
num_test_smiles: 100
