# Logging configuration
experiment_name: mamba
agent_name: chembl28p
logger_backend: wandb # csv, wandb, tensorboard, or null
seed: 101
log_frequency: 500

# Dataset configuration
train_dataset_path: /workspace1/Priors/ChEMBL_potent/processed_data/ChEMBL28p_all_undersample-8.smi.gz
tokenizer: SMILESTokenizerChEMBL # SMILESTokenizer, SMILESTokenizer2, SMILESTokenizer3, DeepSMILESTokenizer, SELFIESTokenizer, AISTokenizer, SAFETokenizer, SmiZipTokenizer
#special_tokens: [".", "/", "\\", "@", "%", "*", "=", ":", "#", ">", "+", "-", "<UNK>", "<SEP>"]
recompute_dataset: True # if False and dataset_log_dir contains a dataset, it will be used
dataset_log_dir: /tmp/pretrain

# Model configuration
model: mamba # gru, lstm, gpt2, mamba or llama2
model_log_dir: mamba/chembl28p

# Training configuration
lr: 0.001
lr_scheduler: StepLR
lr_scheduler_kwargs:
  step_size: 500 # Change every n epochs
  gamma: 0.95 # 1.0 = no decay
epochs: 25
batch_size: 512 # 512
randomize_smiles: True
num_test_smiles: 100
