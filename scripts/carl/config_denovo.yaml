# Logging configuration
experiment_name: MolOpt_reward_shaping
agent_name: reenhance_likely1e2
log_dir: results/reenhance_likely1e2
logger_backend: wandb # csv, wandb, tensorboard, or null
seed: [101, 102, 103, 104, 105]

# Environment configuration
num_envs: 128 # Number of smiles to generate in parallel
total_smiles: 10_000

# Scoring function
molscore: MolOpt
molscore_include: []
custom_task: null # Requires molscore to be set to null

# Fix the beginning of the generated molecules
prompt: null  # e.g. c1ccccc

# Architecture
model: gru # gru, lstm, or gpt2
# The default prior varies for each model. Refer to the README file in the root directory for more information.
# The default vocabulary varies for each prior. Refer to the README file in the root directory for more information.

# Optimizer configuration
lr: 0.0001 # default: 0.0001
lr_annealing: False
eps: 1.0e-08
weight_decay: 0.0

# Hill-Climb
topk: 1.0

# Reinforce configuration
alpha: 1 # 1 Reinforce
sigma: 0.0 # 0.001 # No prior=0, 0.001 - 0.005 for prior
baseline: null # null, mab or loo
likely_penalty: True
likely_penalty_coef: 100
entropy_coef: 0.0 # Default 0.01

# Data replay configuration
experience_replay: False
replay_sampler: uniform # uniform, prioritized
replay_buffer_size: 100
replay_batch_size: 10

